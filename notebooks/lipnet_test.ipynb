{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84671480-18f4-4b8e-ae08-410e60374db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965f380-7968-411c-9c26-ca40cdac2cfd",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b8d8b9-b641-4ae8-b9b3-5e5dc21a6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbff5fd0-46c2-4134-a74d-e16b7feb5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O detector.tflite -q https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b545c755-f785-4637-9ba0-f32a6d8907e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'detector.tflite'\n",
    "IMAGE_FILE = '1405992.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa76171-24cc-43d0-b5b2-ac1ece28a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "options = vision.FaceDetectorOptions(base_options=base_options)\n",
    "detector = vision.FaceDetector.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(IMAGE_FILE)\n",
    "\n",
    "# STEP 4: Detect faces in the input image.\n",
    "detection_result = detector.detect(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eaef72b-908b-467a-a5bb-77c235e0370f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundingBox(origin_x=683, origin_y=215, width=240, height=240)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_result.detections[0].bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "632e27c2-ff95-439d-a15c-2a698aedd9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9633539915084839"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_result.detections[0].categories[0].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee08c6d-e635-443b-a583-2bd79158eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_frame_from_opencv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0637e14e-51dd-48b4-a4e0-5c006f733232",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Task is not initialized with the image mode. Current running mode:VIDEO",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m options \u001b[38;5;241m=\u001b[39m FaceDetectorOptions(\n\u001b[1;32m     10\u001b[0m     base_options\u001b[38;5;241m=\u001b[39mBaseOptions(model_asset_path\u001b[38;5;241m=\u001b[39mmodel_path),\n\u001b[1;32m     11\u001b[0m     running_mode\u001b[38;5;241m=\u001b[39mVisionRunningMode\u001b[38;5;241m.\u001b[39mVIDEO)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m FaceDetector\u001b[38;5;241m.\u001b[39mcreate_from_options(options) \u001b[38;5;28;01mas\u001b[39;00m detector:\n\u001b[0;32m---> 13\u001b[0m     detection_result \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/va/lib/python3.10/site-packages/mediapipe/tasks/python/vision/face_detector.py:217\u001b[0m, in \u001b[0;36mFaceDetector.detect\u001b[0;34m(self, image, image_processing_options)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs face detection on the provided MediaPipe Image.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03mOnly use this method when the FaceDetector is created with the image\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m  RuntimeError: If face detection failed to run.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m normalized_rect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_to_normalized_rect(\n\u001b[1;32m    215\u001b[0m     image_processing_options, image, roi_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    216\u001b[0m )\n\u001b[0;32m--> 217\u001b[0m output_packets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_image_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_IMAGE_IN_STREAM_NAME\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_creator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_NORM_RECT_STREAM_NAME\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_creator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_proto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalized_rect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pb2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_packets[_DETECTIONS_OUT_STREAM_NAME]\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[1;32m    224\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m FaceDetectorResult([])\n",
      "File \u001b[0;32m~/miniconda3/envs/va/lib/python3.10/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:91\u001b[0m, in \u001b[0;36mBaseVisionTaskApi._process_image_data\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A synchronous method to process single image inputs.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mThe call blocks the current thread until a failure status or a successful\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m  ValueError: If the task's running mode is not set to image mode.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode \u001b[38;5;241m!=\u001b[39m _RunningMode\u001b[38;5;241m.\u001b[39mIMAGE:\n\u001b[0;32m---> 91\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is not initialized with the image mode. Current running mode:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     93\u001b[0m       \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     94\u001b[0m   )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner\u001b[38;5;241m.\u001b[39mprocess(inputs)\n",
      "\u001b[0;31mValueError\u001b[0m: Task is not initialized with the image mode. Current running mode:VIDEO"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceDetector = mp.tasks.vision.FaceDetector\n",
    "FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a face detector instance with the video mode:\n",
    "options = FaceDetectorOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.VIDEO)\n",
    "with FaceDetector.create_from_options(options) as detector:\n",
    "    detection_result = detector.detect(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7524e-f1ee-4ed8-90d4-e6bfd96fbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if sys.version_info[0] < 3 and sys.version_info[1] < 2:\n",
    "\traise Exception(\"Must be using >= Python 3.2\")\n",
    "\n",
    "from os import listdir, path\n",
    "\n",
    "if not path.isfile('face_detection/detection/sfd/s3fd.pth'):\n",
    "\traise FileNotFoundError('Save the s3fd model to face_detection/detection/sfd/s3fd.pth \\\n",
    "\t\t\t\t\t\t\tbefore running this script!')\n",
    "\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "import argparse, os, cv2, traceback, subprocess\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import audio\n",
    "from hparams import hparams as hp\n",
    "\n",
    "import face_detection\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--ngpu', help='Number of GPUs across which to run in parallel', default=1, type=int)\n",
    "parser.add_argument('--batch_size', help='Single GPU Face detection batch size', default=32, type=int)\n",
    "parser.add_argument(\"--data_root\", help=\"Root folder of the LRS2 dataset\", required=True)\n",
    "parser.add_argument(\"--preprocessed_root\", help=\"Root folder of the preprocessed dataset\", required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "fa = [face_detection.FaceAlignment(face_detection.LandmarksType._2D, flip_input=False, \n",
    "\t\t\t\t\t\t\t\t\tdevice='cuda:{}'.format(id)) for id in range(args.ngpu)]\n",
    "\n",
    "template = 'ffmpeg -loglevel panic -y -i {} -strict -2 {}'\n",
    "# template2 = 'ffmpeg -hide_banner -loglevel panic -threads 1 -y -i {} -async 1 -ac 1 -vn -acodec pcm_s16le -ar 16000 {}'\n",
    "\n",
    "def process_video_file(vfile, args, gpu_id):\n",
    "\tvideo_stream = cv2.VideoCapture(vfile)\n",
    "\t\n",
    "\tframes = []\n",
    "\twhile 1:\n",
    "\t\tstill_reading, frame = video_stream.read()\n",
    "\t\tif not still_reading:\n",
    "\t\t\tvideo_stream.release()\n",
    "\t\t\tbreak\n",
    "\t\tframes.append(frame)\n",
    "\t\n",
    "\tvidname = os.path.basename(vfile).split('.')[0]\n",
    "\tdirname = vfile.split('/')[-2]\n",
    "\n",
    "\tfulldir = path.join(args.preprocessed_root, dirname, vidname)\n",
    "\tos.makedirs(fulldir, exist_ok=True)\n",
    "\n",
    "\tbatches = [frames[i:i + args.batch_size] for i in range(0, len(frames), args.batch_size)]\n",
    "\n",
    "\ti = -1\n",
    "\tfor fb in batches:\n",
    "\t\tpreds = fa[gpu_id].get_detections_for_batch(np.asarray(fb))\n",
    "\n",
    "\t\tfor j, f in enumerate(preds):\n",
    "\t\t\ti += 1\n",
    "\t\t\tif f is None:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tx1, y1, x2, y2 = f\n",
    "\t\t\tcv2.imwrite(path.join(fulldir, '{}.jpg'.format(i)), fb[j][y1:y2, x1:x2])\n",
    "\n",
    "def process_audio_file(vfile, args):\n",
    "\tvidname = os.path.basename(vfile).split('.')[0]\n",
    "\tdirname = vfile.split('/')[-2]\n",
    "\n",
    "\tfulldir = path.join(args.preprocessed_root, dirname, vidname)\n",
    "\tos.makedirs(fulldir, exist_ok=True)\n",
    "\n",
    "\twavpath = path.join(fulldir, 'audio.wav')\n",
    "\n",
    "\tcommand = template.format(vfile, wavpath)\n",
    "\tsubprocess.call(command, shell=True)\n",
    "\n",
    "\t\n",
    "def mp_handler(job):\n",
    "\tvfile, args, gpu_id = job\n",
    "\ttry:\n",
    "\t\tprocess_video_file(vfile, args, gpu_id)\n",
    "\texcept KeyboardInterrupt:\n",
    "\t\texit(0)\n",
    "\texcept:\n",
    "\t\ttraceback.print_exc()\n",
    "\t\t\n",
    "def main(args):\n",
    "\tprint('Started processing for {} with {} GPUs'.format(args.data_root, args.ngpu))\n",
    "\n",
    "\tfilelist = glob(path.join(args.data_root, '*/*.mp4'))\n",
    "\n",
    "\tjobs = [(vfile, args, i%args.ngpu) for i, vfile in enumerate(filelist)]\n",
    "\tp = ThreadPoolExecutor(args.ngpu)\n",
    "\tfutures = [p.submit(mp_handler, j) for j in jobs]\n",
    "\t_ = [r.result() for r in tqdm(as_completed(futures), total=len(futures))]\n",
    "\n",
    "\tprint('Dumping audios...')\n",
    "\n",
    "\tfor vfile in tqdm(filelist):\n",
    "\t\ttry:\n",
    "\t\t\tprocess_audio_file(vfile, args)\n",
    "\t\texcept KeyboardInterrupt:\n",
    "\t\t\texit(0)\n",
    "\t\texcept:\n",
    "\t\t\ttraceback.print_exc()\n",
    "\t\t\tcontinue\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91adf540-b8d4-43a7-9f6b-5f8fdf68d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_align_file(align_file_path):\n",
    "    timestamps = []\n",
    "    phonetic_labels = []\n",
    "\n",
    "    with open(align_file_path, 'r') as align_file:\n",
    "        for line in align_file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 3:\n",
    "                timestamp = float(parts[0])\n",
    "                label = parts[2]\n",
    "                timestamps.append(timestamp)\n",
    "                phonetic_labels.append(label)\n",
    "\n",
    "    return timestamps, phonetic_labels\n",
    "\n",
    "align_file_path = 'path_to_your_align_file.align'\n",
    "\n",
    "timestamps, phonetic_labels = read_align_file(align_file_path)\n",
    "\n",
    "for timestamp, label in zip(timestamps, phonetic_labels):\n",
    "    print(f\"Timestamp: {timestamp}, Label: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00aca6eb-5087-4f04-b063-33891a0cb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, video_root, align_root):\n",
    "        self.video_root = video_root\n",
    "        self.align_root = align_root\n",
    "\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        samples = []\n",
    "        speakers = os.listdir(self.video_root)\n",
    "        for speaker in speakers:\n",
    "            video_files = os.listdir(os.path.join(self.video_root, speaker))\n",
    "            align_files = os.listdir(os.path.join(self.align_root, speaker))\n",
    "\n",
    "            # Match video files with corresponding align files\n",
    "            for video_file in video_files:\n",
    "                video_id = os.path.splitext(video_file)[0]\n",
    "                if f\"{video_id}.align\" in align_files:\n",
    "                    samples.append((speaker, video_id))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speaker, video_id = self.samples[idx]\n",
    "        video_path = os.path.join(self.video_root, speaker, f\"{video_id}.mpg\")\n",
    "        align_path = os.path.join(self.align_root, speaker, f\"{video_id}.align\")\n",
    "\n",
    "        video, _, _ = read_video(video_path)\n",
    "        timestamps, labels = self._read_align_file(align_path)\n",
    "\n",
    "        return video, timestamps, labels\n",
    "\n",
    "    def _read_align_file(self, align_file_path):\n",
    "        timestamps = []\n",
    "        labels = []\n",
    "\n",
    "        with open(align_file_path, 'r') as align_file:\n",
    "            for line in align_file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 3:\n",
    "                    timestamp = float(parts[0])\n",
    "                    label = parts[2]\n",
    "                    timestamps.append(timestamp)\n",
    "                    labels.append(label)\n",
    "\n",
    "        return timestamps, labels\n",
    "\n",
    "video_root = '/home2/souvikg544/gridcorpus/video'\n",
    "align_root = '/home2/souvikg544/gridcorpus/transcription'\n",
    "\n",
    "dataset = CustomDataset(video_root, align_root)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Example usage of DataLoader\n",
    "# for video, timestamps, labels in dataloader:\n",
    "#     # Process the video frames and labels here\n",
    "#     print(f\"Video shape: {video.shape}, Timestamps: {timestamps}, Labels: {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba65fa-1025-43d3-9656-948d510faf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_video\n",
    "from torchvision import transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, video_root, align_root, transform=None):\n",
    "        self.video_root = video_root\n",
    "        self.align_root = align_root\n",
    "        self.transform = transform\n",
    "\n",
    "        self.samples = self._prepare_samples()\n",
    "\n",
    "     def _prepare_samples(self):\n",
    "        samples = []\n",
    "        speakers = os.listdir(self.video_root)\n",
    "        for speaker in speakers:\n",
    "            video_files = os.listdir(os.path.join(self.video_root, speaker))\n",
    "            align_files = os.listdir(os.path.join(self.align_root, speaker))\n",
    "\n",
    "            # Match video files with corresponding align files\n",
    "            for video_file in video_files:\n",
    "                video_id = os.path.splitext(video_file)[0]\n",
    "                if f\"{video_id}.align\" in align_files:\n",
    "                    samples.append((speaker, video_id))\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        speaker, video_id = self.samples[idx]\n",
    "        video_path = os.path.join(self.video_root, speaker, f\"{video_id}.mpg\")\n",
    "        align_path = os.path.join(self.align_root, speaker, f\"{video_id}.align\")\n",
    "\n",
    "        video, _, _ = read_video(video_path)\n",
    "        timestamps, labels = self._read_align_file(align_path)\n",
    "\n",
    "        frame_tensors = []\n",
    "        for timestamp in timestamps:\n",
    "            frame_idx = int(timestamp * video.shape[0])\n",
    "            frame = video[frame_idx]\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frame_tensors.append(frame)\n",
    "\n",
    "        frame_tensors = torch.stack(frame_tensors).to(torch.device(\"cuda\"))  # Move to GPU\n",
    "\n",
    "        label_tensors = torch.tensor(labels).to(torch.device(\"cuda\"))  # Move to GPU\n",
    "\n",
    "        return frame_tensors, label_tensors\n",
    "\n",
    "\n",
    "     def _read_align_file(self, align_file_path):\n",
    "        timestamps = []\n",
    "        labels = []\n",
    "\n",
    "        with open(align_file_path, 'r') as align_file:\n",
    "            for line in align_file:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 3:\n",
    "                    timestamp = float(parts[0])\n",
    "                    label = parts[2]\n",
    "                    timestamps.append(timestamp)\n",
    "                    labels.append(label)\n",
    "\n",
    "        return timestamps, labels\n",
    "\n",
    "# Set your image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "video_root = '/home2/souvikg544/gridcorpus/video'\n",
    "align_root = '/home2/souvikg544/gridcorpus/transcription'\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = CustomDataset(video_root, align_root, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Example usage of DataLoader\n",
    "for frames, labels in dataloader:\n",
    "    frames = frames.to(torch.device(\"cuda\"))  # Move to GPU\n",
    "    labels = labels.to(torch.device(\"cuda\"))  # Move to GPU\n",
    "\n",
    "    # Process frames and labels on GPU\n",
    "    print(f\"Frames shape: {frames.shape}, Labels: {labels}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "va",
   "language": "python",
   "name": "va"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
